{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanadG123/DaSH-Lab-Assignment-2024/blob/main/Copy_of_SOP_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Connecting to Google Drive in Colab\n",
        "\n",
        "To access files stored in your Google Drive within a Google Colab notebook, you need to mount your Drive to the Colab environment.  This allows the notebook to interact with your Drive files as if they were local.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Authorization:**  The first time you connect, you'll be prompted to authorize Colab to access your Google Drive.  You'll need to follow the provided link, which will open a new window in your browser, and grant permission.  A unique authorization code will be displayed, which you'll then need to enter into the Colab notebook to complete the connection.\n",
        "\n",
        "2. **Mounting:** After successful authorization, your Google Drive will be mounted to a specific directory within the Colab runtime environment.  You can then use standard file system operations (like `os.listdir`, `open`, etc.) to access your files.\n",
        "\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Security:**  Be mindful of the files you choose to share with Colab.  Only grant access to the files you absolutely need for your current project.\n",
        "\n",
        "* **Runtime:** The mounted Drive connection is specific to the current Colab runtime. If you reset your runtime or restart your notebook, you'll need to remount your Drive.\n",
        "\n",
        "* **File Paths:** Pay careful attention to the file paths when referencing your Drive files. They will not be in your typical Drive location. You'll need to use the path provided by the mounting command, typically something like `/content/drive/My Drive/`.\n",
        "\n",
        "\n",
        "**Example (Conceptual):**\n",
        "\n",
        "After successful mounting, a file located at `My Drive/data/my_file.csv` on your Google Drive would be accessible at `/content/drive/My Drive/data/my_file.csv` within your Colab notebook.\n"
      ],
      "metadata": {
        "id": "RsIVUYWud4sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAiHpXL8dd2y",
        "outputId": "d5f125e4-c39f-4f0c-da75-5ba19f6bb640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure GPU Access: Configuring Google Colab to Use GPU\n",
        "\n",
        "To use the `GPUDevice` class effectively, the code requires access to an **NVIDIA GPU** with **NVML (NVIDIA Management Library) support**. If you're working in **Google Colab**, follow these steps to change the runtime to GPU and ensure the environment is ready for GPU-based operations.\n",
        "\n",
        "#### Steps to Enable GPU Runtime in Google Colab:\n",
        "1. **Open Google Colab Notebook:**\n",
        "   - Navigate to [Google Colab](https://colab.research.google.com) and open a new or existing notebook.\n",
        "\n",
        "2. **Change the Runtime to GPU:**\n",
        "   - Click on the **Runtime** menu at the top of the notebook.\n",
        "   - Select **Change runtime type** from the dropdown.\n",
        "   - In the dialog box, under **Hardware accelerator**, choose **GPU**.\n",
        "   - Click **Save**.\n",
        "\n",
        "3. **Verify GPU Availability:**\n",
        "   - Run the following code snippet in a new Colab cell to confirm that the GPU is available:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "     ```\n",
        "   - The output should list one or more GPUs if the runtime is correctly set up.\n",
        "\n",
        "4. **Check for NVIDIA NVML Support:**\n",
        "   - Google Colab provides access to NVIDIA GPUs, but you need to ensure **NVML** is installed and working. Colab typically includes NVML by default, but you can verify it by running:\n",
        "     ```python\n",
        "     !nvidia-smi\n",
        "     ```\n",
        "   - This command shows the **GPU status** and verifies if NVML is accessible.\n",
        "\n",
        "5. **Install Missing Packages (if required):**\n",
        "   - If you encounter missing modules like `pynvml`, install it by running:\n",
        "     ```python\n",
        "     !pip install nvidia-ml-py3\n",
        "     ```\n",
        "\n",
        "6. **Restart the Runtime:**\n",
        "   - After setting the runtime to GPU and installing required libraries, restart the Colab runtime by clicking **Runtime > Restart runtime**. This ensures the GPU environment is refreshed and ready for use.\n",
        "\n",
        "7. **Test NVML Initialization:**\n",
        "   - You can now create an instance of the `GPUDevice` class to confirm NVML initialization:\n",
        "     ```python\n",
        "     from pynvml import nvmlInit, nvmlShutdown\n",
        "\n",
        "     try:\n",
        "         nvmlInit()\n",
        "         print(\"NVML initialized successfully!\")\n",
        "         nvmlShutdown()\n",
        "     except Exception as e:\n",
        "         print(f\"NVML failed to initialize: {e}\")\n",
        "     ```\n",
        "\n",
        "### Key Notes on GPU Access:\n",
        "- **Compatibility:** Your code will only work on NVIDIA GPUs, as NVML is designed specifically for NVIDIA hardware.\n",
        "- **Limited GPU Access:** On Colab, the GPU usage is time-limited, so be mindful of runtime restrictions, especially if running long experiments.\n",
        "- **Runtime Environment:** Ensure that your code executes within the **GPU runtime environment** to leverage hardware acceleration properly.\n",
        "\n",
        "By following these steps, you ensure that Colab is configured correctly for running your `GPUDevice` class, which relies on GPU metrics gathered through NVML."
      ],
      "metadata": {
        "id": "TuOwJCR2e7Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available,\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "bNEKuUBhU_Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162dde3b-f517-4fb2-b494-4feb29cf4845"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available, [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unE3kjLlWFHS",
        "outputId": "2aaf7e33-b6a5-4c4e-f4a7-bdf82283eedf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 31 08:52:04 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-ml-py3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jrFM5aSYTui",
        "outputId": "6b444981-c6e4-44b9-a9f2-94c3dd1013bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-ml-py3\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-ml-py3\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=d373561b2b9021ee9ea9a8151ddb5178cafd11455efbd4679db10874e5473c07\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "Successfully built nvidia-ml-py3\n",
            "Installing collected packages: nvidia-ml-py3\n",
            "Successfully installed nvidia-ml-py3-7.352.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import nvmlInit, nvmlShutdown\n",
        "\n",
        "try:\n",
        "    nvmlInit()\n",
        "    print(\"NVML initialized successfully!\")\n",
        "    nvmlShutdown()\n",
        "except Exception as e:\n",
        "    print(f\"NVML failed to initialize: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YvSVcLlXhWi",
        "outputId": "9657619e-9b11-4ed4-bedd-933c73090f1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVML initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Use the `GPUDevice` Class\n",
        "\n",
        "This guide provides instructions on using the `GPUDevice` class to monitor GPU metrics such as power consumption, temperature, and memory usage using the NVIDIA Management Library (NVML).\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Prerequisites**\n",
        "\n",
        "- **Install NVML Python Bindings**: Make sure the `pynvml` library is installed. If not, install it using:\n",
        "  ```bash\n",
        "  !pip install nvidia-ml-py3\n",
        "  ```\n",
        "- **Ensure GPU Access**: The code needs access to an NVIDIA GPU with NVML support.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Class Overview**\n",
        "\n",
        "The `GPUDevice` class monitors various GPU metrics. It collects data periodically and saves it in a CSV file for further analysis. Below are the key methods:\n",
        "\n",
        "| **Method**                  | **Description**                                    |\n",
        "|-----------------------------|----------------------------------------------------|\n",
        "| `__init__`                  | Initializes NVML and prepares the GPU object.      |\n",
        "| `get_power()`               | Returns the GPU's current power usage (Watts).     |\n",
        "| `get_energy()`              | Returns the total energy consumption (Joules).     |\n",
        "| `get_temp()`                | Retrieves the GPU temperature (Celsius).           |\n",
        "| `get_memory_usage()`        | Reports memory usage in MB.                        |\n",
        "| `get_gpu_utilization()`     | Returns the GPU utilization percentage.            |\n",
        "| `get_graphics_clock()`      | Retrieves the current graphics clock (MHz).        |\n",
        "| `get_memory_clock()`        | Retrieves the current memory clock (MHz).          |\n",
        "| `get_pcie_throughput()`     | Reports PCIe throughput (KB/s).                    |\n",
        "| `start_reading()`           | Starts collecting metrics periodically.            |\n",
        "| `stop_reading()`            | Stops data collection and saves it to CSV.         |\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Setting Up the GPU Monitor**\n",
        "\n",
        "```python\n",
        "# Initialize the GPU monitor\n",
        "gpu_device = GPUDevice(\n",
        "    device_index=0,                # Index of the GPU (if multiple GPUs available)\n",
        "    kernel_name=\"Example Kernel\",  # Name to tag data with\n",
        "    sampling_interval=0.1,         # Time interval between each sample (in seconds)\n",
        "    log_file=\"gpu_monitor.log\"     # Log file to store runtime data\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Starting and Stopping the Monitoring Process**\n",
        "\n",
        "Start data collection by calling `start_reading()` and stop it using `stop_reading()`.\n",
        "\n",
        "```python\n",
        "# Start monitoring GPU metrics\n",
        "gpu_device.start_reading()\n",
        "\n",
        "# Keep collecting data for 10 seconds\n",
        "time.sleep(10)\n",
        "\n",
        "# Stop monitoring and save the results to CSV\n",
        "gpu_device.stop_reading()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **CSV Output and Logging**\n",
        "\n",
        "- **CSV File**: After stopping the data collection, a CSV file will be generated with metrics like temperature, power, memory usage, and GPU utilization.\n",
        "- **Logging**: Data is also logged into the specified log file (`gpu_monitor.log`), which can help with debugging.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Sample Output Format**\n",
        "\n",
        "The generated CSV will contain the following columns:\n",
        "\n",
        "| **Kernel** | **Time (s)** | **Temperature (C)** | **Power (W)** | **Memory Usage (MB)** | **GPU Utilization (%)** | **Graphics Clock (MHz)** | **Memory Clock (MHz)** | **PCIe Tx Throughput (KB/s)** | **PCIe Rx Throughput (KB/s)** |\n",
        "|------------|--------------|---------------------|--------------|----------------------|-------------------------|--------------------------|------------------------|------------------------------|------------------------------|\n",
        "| Example    | 0.1          | 50                  | 75           | 2000                 | 80                      | 1500                     | 700                    | 50                           | 40                           |\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Troubleshooting**\n",
        "\n",
        "- **NVML Initialization Error**: If you encounter errors related to NVML, ensure the NVIDIA drivers are properly installed and the GPU supports NVML.\n",
        "- **Permission Issues**: Running the code may require administrative or root privileges if accessing GPU hardware metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Example Use Case**  \n",
        "\n",
        "```python\n",
        "from gpu_device import GPUDevice\n",
        "\n",
        "# Initialize the GPUDevice instance\n",
        "gpu_device = GPUDevice(\n",
        "    device_index=0,                # GPU index (if multiple GPUs)\n",
        "    kernel_name=\"Example Kernel\",  # Tag for collected data\n",
        "    sampling_interval=0.1,         # Interval between samples (in seconds)\n",
        "    log_file=\"gpu_monitor.log\"     # Log file for runtime data\n",
        ")\n",
        "\n",
        "gpu_device.start_reading()  # Start monitoring\n",
        "\n",
        "# << Insert your GPU code here >>  \n",
        "# Example: Simple TensorFlow GPU computation  \n",
        "import tensorflow as tf  \n",
        "with tf.device('/GPU:0'):\n",
        "    a = tf.random.uniform((1000, 1000))\n",
        "    b = tf.random.uniform((1000, 1000))\n",
        "    c = tf.matmul(a, b)\n",
        "\n",
        "gpu_device.stop_reading()  # Stop monitoring and save data\n",
        "```\n",
        "\n",
        "This monitors GPU metrics during code execution and generates a CSV file named `gpu_data_Example Kernel.csv`.\n",
        "\n",
        "### 9. **Conclusion**\n",
        "\n",
        "The `GPUDevice` class provides a comprehensive way to monitor GPU metrics, making it useful for tasks such as performance tuning, model optimization, or energy efficiency studies. You can extend the class by adding more NVML metrics or customizing the output format."
      ],
      "metadata": {
        "id": "tkrteMg4eQSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tc1agNufvFz",
        "outputId": "0c29dbe8-9178-40fa-d23b-607ba59341dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 31 08:52:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NVIDIA System Management Interface (nvidia-smi)** is a command-line utility that provides a comprehensive interface for monitoring and managing NVIDIA GPUs. It enables users to obtain information about GPU utilization, memory usage, temperature, and power consumption.\n",
        "\n",
        "### Key Functions of nvidia-smi:\n",
        "\n",
        "1. **Monitoring GPU Metrics**:  \n",
        "   - Displays real-time statistics about GPU performance, including GPU and memory usage, temperature, and power consumption.\n",
        "\n",
        "2. **Process Management**:  \n",
        "   - Lists processes currently using the GPU, along with their memory consumption and GPU utilization, allowing users to identify resource-intensive applications.\n",
        "\n",
        "3. **Changing GPU Parameters**:  \n",
        "   - Users can modify several GPU settings, including power limits, clock speeds, and performance modes. This can be done with commands like:\n",
        "     - **Set Power Limit**:  \n",
        "       ```bash\n",
        "       nvidia-smi -i <gpu_index> -pl <power_limit_in_Watts>\n",
        "       ```\n",
        "     - **Set Performance Mode**:  \n",
        "       ```bash\n",
        "       nvidia-smi -i <gpu_index> -pm <mode>  # where mode can be 0 (disabled) or 1 (enabled)\n",
        "       ```\n",
        "\n",
        "### Additional Resources\n",
        "For more detailed information and examples, you can refer to the official NVIDIA documentation: [NVIDIA-SMI Documentation](https://docs.nvidia.com/deploy/pdf/NVSMI_Manual.pdf)."
      ],
      "metadata": {
        "id": "Sz2LtiLdgplS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## NVIDIA-SMI Documentation:- https://docs.nvidia.com/deploy/pdf/NVSMI_Manual.pdf"
      ],
      "metadata": {
        "id": "TW5tOJlegvDm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: GPU Feature Exploration and Data Collection using `nvidia-smi`\n",
        "\n",
        "This task consists of two parts: first, identifying adjustable GPU features using the `nvidia-smi` command, and second, running GPU-intensive applications to collect performance data by **modifying GPU features**. The **model you use is not important**, but the **data collected** during these runs is essential.\n",
        "\n",
        "---\n",
        "\n",
        "## **Task 1: Explore `nvidia-smi` and Identify Adjustable GPU Features**  \n",
        "\n",
        "### **Instructions:**  \n",
        "1. **Explore the `nvidia-smi` Command:**  \n",
        "   - Use the command line to investigate the functionality of `nvidia-smi`.  \n",
        "   - Document **available subcommands** for querying and setting features.\n",
        "   - Example:\n",
        "     ```bash\n",
        "     nvidia-smi --help  # To explore available commands\n",
        "     nvidia-smi --query-gpu=all --format=csv  # Query all GPU metrics\n",
        "     ```\n",
        "\n",
        "2. **Identify Adjustable Features:**  \n",
        "   - List **all adjustable features** such as:\n",
        "     - Power Limits\n",
        "     - Memory Clocks\n",
        "     - Fan Speed\n",
        "     - Compute Modes  \n",
        "   - **Example Command:**  \n",
        "     ```bash\n",
        "     nvidia-smi -pl 150  # Set power limit to 150W\n",
        "     ```\n",
        "     \n",
        "3. **Output:**  \n",
        "   - **Document each feature** in Markdown with the following details:\n",
        "     - **Feature Name:** e.g., Power Limit\n",
        "     - **Possible Values/Settings:** e.g., 100W - 300W\n",
        "     - **Command to Modify:** e.g., `nvidia-smi -pl <value>`  \n",
        "     - **Effect:** e.g., Impacts power draw and temperature regulation\n",
        "\n",
        "---\n",
        "\n",
        "## **Task 2: Run GPU-Intensive Applications and Collect Data with GPU Feature Modifications**  \n",
        "\n",
        "While the **specific model** you use (e.g., Ultralytics or Hugging Face) is not important, the **data collected during feature changes** is critical. You need to **modify GPU features** identified in Task 1 and **measure performance metrics** during the execution of each task.\n",
        "\n",
        "### **Instructions:**  \n",
        "1. **Run GPU-Intensive Applications:**\n",
        "   - Choose two different applications, such as:\n",
        "     - **Ultralytics Model Training:**  \n",
        "\n",
        "     - **Fine-tuning Hugging Face LLM:**  \n",
        "\n",
        "2. **Modify GPU Features:**\n",
        "   - For **each identified feature** (e.g., power limit, fan speed), modify its value.\n",
        "   - **Example Command:**  \n",
        "     ```bash\n",
        "     nvidia-smi -pl 150  # Set power limit to 150W\n",
        "     ```\n",
        "\n",
        "3. **Collect Performance Data using `GPUDevice` Class:**  \n",
        "   - Capture the following **GPU metrics** using GPUDevice Class as explained above.\n",
        "\n",
        "4. **Repeat the Process for Each Feature Configuration:**\n",
        "   - For **each feature change**, re-run the selected task and **log the collected data**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion:**\n",
        "Summarize key findings, such as:\n",
        "- Which feature changes had the most significant impact on performance, power consumption, or temperature.\n",
        "- Any patterns observed (e.g., higher memory clocks improve model training speed but increase temperature).\n",
        "\n",
        "---\n",
        "\n",
        "## **Deliverables:**\n",
        "1. **List of adjustable GPU features** with explanations.\n",
        "2. **Collected data** for each feature change with applications.\n",
        "\n",
        "---\n",
        "\n",
        "This task ensures that you learn how to **use `nvidia-smi` to manipulate GPU configurations** and **analyze their impact on performance**. Focus on the **data collected** and ensure all findings are well-documented for future reference."
      ],
      "metadata": {
        "id": "ftT-5I4OxqRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWBTz-yIz3kZ",
        "outputId": "a08ddaf4-e81f-4dc6-be3f-006fc31778bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA System Management Interface -- v535.104.05\n",
            "\n",
            "NVSMI provides monitoring information for Tesla and select Quadro devices.\n",
            "The data is presented in either a plain text or an XML format, via stdout or a file.\n",
            "NVSMI also provides several management operations for changing the device state.\n",
            "\n",
            "Note that the functionality of NVSMI is exposed through the NVML C-based\n",
            "library. See the NVIDIA developer website for more information about NVML.\n",
            "Python wrappers to NVML are also available.  The output of NVSMI is\n",
            "not guaranteed to be backwards compatible; NVML and the bindings are backwards\n",
            "compatible.\n",
            "\n",
            "http://developer.nvidia.com/nvidia-management-library-nvml/\n",
            "http://pypi.python.org/pypi/nvidia-ml-py/\n",
            "Supported products:\n",
            "- Full Support\n",
            "    - All Tesla products, starting with the Kepler architecture\n",
            "    - All Quadro products, starting with the Kepler architecture\n",
            "    - All GRID products, starting with the Kepler architecture\n",
            "    - GeForce Titan products, starting with the Kepler architecture\n",
            "- Limited Support\n",
            "    - All Geforce products, starting with the Kepler architecture\n",
            "nvidia-smi [OPTION1 [ARG1]] [OPTION2 [ARG2]] ...\n",
            "\n",
            "    -h,   --help                Print usage information and exit.\n",
            "\n",
            "  LIST OPTIONS:\n",
            "\n",
            "    -L,   --list-gpus           Display a list of GPUs connected to the system.\n",
            "\n",
            "    -B,   --list-excluded-gpus  Display a list of excluded GPUs in the system.\n",
            "\n",
            "  SUMMARY OPTIONS:\n",
            "\n",
            "    <no arguments>              Show a summary of GPUs connected to the system.\n",
            "\n",
            "    [plus any of]\n",
            "\n",
            "    -i,   --id=                 Target a specific GPU.\n",
            "    -f,   --filename=           Log to a specified file, rather than to stdout.\n",
            "    -l,   --loop=               Probe until Ctrl+C at specified second interval.\n",
            "\n",
            "  QUERY OPTIONS:\n",
            "\n",
            "    -q,   --query               Display GPU or Unit info.\n",
            "\n",
            "    [plus any of]\n",
            "\n",
            "    -u,   --unit                Show unit, rather than GPU, attributes.\n",
            "    -i,   --id=                 Target a specific GPU or Unit.\n",
            "    -f,   --filename=           Log to a specified file, rather than to stdout.\n",
            "    -x,   --xml-format          Produce XML output.\n",
            "          --dtd                 When showing xml output, embed DTD.\n",
            "    -d,   --display=            Display only selected information: MEMORY,\n",
            "                                    UTILIZATION, ECC, TEMPERATURE, POWER, CLOCK,\n",
            "                                    COMPUTE, PIDS, PERFORMANCE, SUPPORTED_CLOCKS,\n",
            "                                    PAGE_RETIREMENT, ACCOUNTING, ENCODER_STATS,\n",
            "                                    SUPPORTED_GPU_TARGET_TEMP, VOLTAGE\n",
            "                                    FBC_STATS, ROW_REMAPPER, RESET_STATUS\n",
            "                                Flags can be combined with comma e.g. ECC,POWER.\n",
            "                                Sampling data with max/min/avg is also returned \n",
            "                                for POWER, UTILIZATION and CLOCK display types.\n",
            "                                Doesn't work with -u or -x flags.\n",
            "    -l,   --loop=               Probe until Ctrl+C at specified second interval.\n",
            "\n",
            "    -lms, --loop-ms=            Probe until Ctrl+C at specified millisecond interval.\n",
            "\n",
            "  SELECTIVE QUERY OPTIONS:\n",
            "\n",
            "    Allows the caller to pass an explicit list of properties to query.\n",
            "\n",
            "    [one of]\n",
            "\n",
            "    --query-gpu                 Information about GPU.\n",
            "                                Call --help-query-gpu for more info.\n",
            "    --query-supported-clocks    List of supported clocks.\n",
            "                                Call --help-query-supported-clocks for more info.\n",
            "    --query-compute-apps        List of currently active compute processes.\n",
            "                                Call --help-query-compute-apps for more info.\n",
            "    --query-accounted-apps      List of accounted compute processes.\n",
            "                                Call --help-query-accounted-apps for more info.\n",
            "                                This query is not supported on vGPU host.\n",
            "    --query-retired-pages       List of device memory pages that have been retired.\n",
            "                                Call --help-query-retired-pages for more info.\n",
            "    --query-remapped-rows       Information about remapped rows.\n",
            "                                Call --help-query-remapped-rows for more info.\n",
            "\n",
            "    [mandatory]\n",
            "\n",
            "    --format=                   Comma separated list of format options:\n",
            "                                  csv - comma separated values (MANDATORY)\n",
            "                                  noheader - skip the first line with column headers\n",
            "                                  nounits - don't print units for numerical\n",
            "                                             values\n",
            "\n",
            "    [plus any of]\n",
            "\n",
            "    -i,   --id=                 Target a specific GPU or Unit.\n",
            "    -f,   --filename=           Log to a specified file, rather than to stdout.\n",
            "    -l,   --loop=               Probe until Ctrl+C at specified second interval.\n",
            "    -lms, --loop-ms=            Probe until Ctrl+C at specified millisecond interval.\n",
            "\n",
            "  DEVICE MODIFICATION OPTIONS:\n",
            "\n",
            "    [any one of]\n",
            "\n",
            "    -pm,  --persistence-mode=   Set persistence mode: 0/DISABLED, 1/ENABLED\n",
            "    -e,   --ecc-config=         Toggle ECC support: 0/DISABLED, 1/ENABLED\n",
            "    -p,   --reset-ecc-errors=   Reset ECC error counts: 0/VOLATILE, 1/AGGREGATE\n",
            "    -c,   --compute-mode=       Set MODE for compute applications:\n",
            "                                0/DEFAULT, 1/EXCLUSIVE_THREAD (DEPRECATED),\n",
            "                                2/PROHIBITED, 3/EXCLUSIVE_PROCESS\n",
            "          --gom=                Set GPU Operation Mode:\n",
            "                                    0/ALL_ON, 1/COMPUTE, 2/LOW_DP\n",
            "    -r    --gpu-reset           Trigger reset of the GPU.\n",
            "                                Can be used to reset the GPU HW state in situations\n",
            "                                that would otherwise require a machine reboot.\n",
            "                                Typically useful if a double bit ECC error has\n",
            "                                occurred.\n",
            "                                Reset operations are not guarenteed to work in\n",
            "                                all cases and should be used with caution.\n",
            "    -vm   --virt-mode=          Switch GPU Virtualization Mode:\n",
            "                                Sets GPU virtualization mode to 3/VGPU or 4/VSGA\n",
            "                                Virtualization mode of a GPU can only be set when\n",
            "                                it is running on a hypervisor.\n",
            "    -lgc  --lock-gpu-clocks=    Specifies <minGpuClock,maxGpuClock> clocks as a\n",
            "                                    pair (e.g. 1500,1500) that defines the range \n",
            "                                    of desired locked GPU clock speed in MHz.\n",
            "                                    Setting this will supercede application clocks\n",
            "                                    and take effect regardless if an app is running.\n",
            "                                    Input can also be a singular desired clock value\n",
            "                                    (e.g. <GpuClockValue>). Optionally, --mode can be\n",
            "                                    specified to indicate a special mode.\n",
            "    -m    --mode=               Specifies the mode for --locked-gpu-clocks.\n",
            "                                    Valid modes: 0, 1\n",
            "    -rgc  --reset-gpu-clocks\n",
            "                                Resets the Gpu clocks to the default values.\n",
            "    -lmc  --lock-memory-clocks=  Specifies <minMemClock,maxMemClock> clocks as a\n",
            "                                    pair (e.g. 5100,5100) that defines the range \n",
            "                                    of desired locked Memory clock speed in MHz.\n",
            "                                    Input can also be a singular desired clock value\n",
            "                                    (e.g. <MemClockValue>).\n",
            "    -rmc  --reset-memory-clocks\n",
            "                                Resets the Memory clocks to the default values.\n",
            "    -lmcd --lock-memory-clocks-deferred=\n",
            "                                    Specifies memClock clock to lock. This limit is\n",
            "                                    applied the next time GPU is initialized.\n",
            "                                    This is guaranteed by unloading and reloading the kernel module.\n",
            "                                    Requires root.\n",
            "    -rmcd --reset-memory-clocks-deferred\n",
            "                                Resets the deferred Memory clocks applied.\n",
            "    -ac   --applications-clocks= Specifies <memory,graphics> clocks as a\n",
            "                                    pair (e.g. 2000,800) that defines GPU's\n",
            "                                    speed in MHz while running applications on a GPU.\n",
            "    -rac  --reset-applications-clocks\n",
            "                                Resets the applications clocks to the default values.\n",
            "    -pl   --power-limit=        Specifies maximum power management limit in watts.\n",
            "                                Takes an optional argument --scope.\n",
            "    -sc   --scope=              Specifies the device type for --scope: 0/GPU, 1/TOTAL_MODULE (Grace Hopper Only)\n",
            "    -cc   --cuda-clocks=        Overrides or restores default CUDA clocks.\n",
            "                                In override mode, GPU clocks higher frequencies when running CUDA applications.\n",
            "                                Only on supported devices starting from the Volta series.\n",
            "                                Requires administrator privileges.\n",
            "                                0/RESTORE_DEFAULT, 1/OVERRIDE\n",
            "    -am   --accounting-mode=    Enable or disable Accounting Mode: 0/DISABLED, 1/ENABLED\n",
            "    -caa  --clear-accounted-apps\n",
            "                                Clears all the accounted PIDs in the buffer.\n",
            "          --auto-boost-default= Set the default auto boost policy to 0/DISABLED\n",
            "                                or 1/ENABLED, enforcing the change only after the\n",
            "                                last boost client has exited.\n",
            "          --auto-boost-permission=\n",
            "                                Allow non-admin/root control over auto boost mode:\n",
            "                                0/UNRESTRICTED, 1/RESTRICTED\n",
            "    -mig  --multi-instance-gpu= Enable or disable Multi Instance GPU: 0/DISABLED, 1/ENABLED\n",
            "                                Requires root.\n",
            "    -gtt  --gpu-target-temp=    Set GPU Target Temperature for a GPU in degree celsius.\n",
            "                                Requires administrator privileges\n",
            "\n",
            "   [plus optional]\n",
            "\n",
            "    -i,   --id=                 Target a specific GPU.\n",
            "    -eow, --error-on-warning    Return a non-zero error for warnings.\n",
            "\n",
            "  UNIT MODIFICATION OPTIONS:\n",
            "\n",
            "    -t,   --toggle-led=         Set Unit LED state: 0/GREEN, 1/AMBER\n",
            "\n",
            "   [plus optional]\n",
            "\n",
            "    -i,   --id=                 Target a specific Unit.\n",
            "\n",
            "  SHOW DTD OPTIONS:\n",
            "\n",
            "          --dtd                 Print device DTD and exit.\n",
            "\n",
            "     [plus optional]\n",
            "\n",
            "    -f,   --filename=           Log to a specified file, rather than to stdout.\n",
            "    -u,   --unit                Show unit, rather than device, DTD.\n",
            "\n",
            "    --debug=                    Log encrypted debug information to a specified file. \n",
            "\n",
            " Device Monitoring:\n",
            "    dmon                        Displays device stats in scrolling format.\n",
            "                                \"nvidia-smi dmon -h\" for more information.\n",
            "\n",
            "    daemon                      Runs in background and monitor devices as a daemon process.\n",
            "                                This is an experimental feature. Not supported on Windows baremetal\n",
            "                                \"nvidia-smi daemon -h\" for more information.\n",
            "\n",
            "    replay                      Used to replay/extract the persistent stats generated by daemon.\n",
            "                                This is an experimental feature.\n",
            "                                \"nvidia-smi replay -h\" for more information.\n",
            "\n",
            " Process Monitoring:\n",
            "    pmon                        Displays process stats in scrolling format.\n",
            "                                \"nvidia-smi pmon -h\" for more information.\n",
            "\n",
            " TOPOLOGY:\n",
            "    topo                        Displays device/system topology. \"nvidia-smi topo -h\" for more information.\n",
            "\n",
            " DRAIN STATES:\n",
            "    drain                       Displays/modifies GPU drain states for power idling. \"nvidia-smi drain -h\" for more information.\n",
            "\n",
            " NVLINK:\n",
            "    nvlink                      Displays device nvlink information. \"nvidia-smi nvlink -h\" for more information.\n",
            "\n",
            " C2C:\n",
            "    c2c                         Displays device C2C information. \"nvidia-smi c2c -h\" for more information.\n",
            "\n",
            " CLOCKS:\n",
            "    clocks                      Control and query clock information. \"nvidia-smi clocks -h\" for more information.\n",
            "\n",
            " ENCODER SESSIONS:\n",
            "    encodersessions             Displays device encoder sessions information. \"nvidia-smi encodersessions -h\" for more information.\n",
            "\n",
            " FBC SESSIONS:\n",
            "    fbcsessions                 Displays device FBC sessions information. \"nvidia-smi fbcsessions -h\" for more information.\n",
            "\n",
            " GRID vGPU:\n",
            "    vgpu                        Displays vGPU information. \"nvidia-smi vgpu -h\" for more information.\n",
            "\n",
            " MIG:\n",
            "    mig                         Provides controls for MIG management. \"nvidia-smi mig -h\" for more information.\n",
            "\n",
            " COMPUTE POLICY:\n",
            "    compute-policy              Control and query compute policies. \"nvidia-smi compute-policy -h\" for more information. \n",
            "\n",
            " BOOST SLIDER:\n",
            "    boost-slider                Control and query boost sliders. \"nvidia-smi boost-slider -h\" for more information. \n",
            "\n",
            " POWER HINT:    power-hint                  Estimates GPU power usage. \"nvidia-smi power-hint -h\" for more information. \n",
            "\n",
            " BASE CLOCKS:    base-clocks                 Query GPU base clocks. \"nvidia-smi base-clocks -h\" for more information. \n",
            "\n",
            " CONFIDENTIAL COMPUTE:\n",
            "    conf-compute                Control and query confidential compute. \"nvidia-smi conf-compute -h\" for more information. \n",
            "\n",
            " GPU PERFORMANCE MONITORING: \n",
            "    gpm                         Control and query GPU performance monitoring unit. \"nvidia-smi gpm -h\" for more information. \n",
            "\n",
            "Please see the nvidia-smi(1) manual page for more detailed information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Documentation**\n",
        "##  Querying Commands\n",
        "  1.\tName: The name of the GPU.\n",
        "  2.\tUUID: The unique identifier for the GPU.\n",
        "  3.\tGPU Bus ID: The PCI bus ID of the GPU.\n",
        "  4.\tPersistence Mode: Indicates whether persistence mode is enabled or disabled.\n",
        "  5.\tTemperature (C): The current temperature of the GPU.\n",
        "  6.\tUtilization (%): The current utilization of the GPU (in percentage).\n",
        "  7.\tMemory Usage:\n",
        "  \ta) Total Memory: Total GPU memory; b)\n",
        "\t  Used Memory: Memory currently in use; c)\n",
        "\t  Free Memory: Available memory.\n",
        "  8.\tCompute Mode: The compute mode of the GPU.\n",
        "  9.\tDriver Version: The version of the installed NVIDIA driver.\n",
        "  10.\tDisplay Active: Indicates if the display is active on the GPU.\n",
        "  11.\tFB Memory Usage: Framebuffer memory usage.\n",
        "  12.\tBar1 Memory Usage: Memory usage for BAR1.\n",
        "  13.\tPower Draw (W): Current power consumption of the GPU.\n",
        "  14.\tPower Limit (W): The maximum power limit set for the GPU.\n",
        "  15.\tGPU Instance ID: For GPUs that support Multi-Instance GPU (MIG).\n",
        "  16.\tMigration State: The migration state for the GPU.\n",
        "  17.\tFan Speed (%): Current speed of the GPU fan.\n",
        "  18.\tPerformance State: The current performance state of the GPU (P-State).\n",
        "  19.\tEncoder Utilization (%): Utilization of the GPU encoder.\n",
        "  20.\tDecoder Utilization (%): Utilization of the GPU decoder.\n",
        "  21.\tEcc Mode: Indicates whether ECC (Error-Correcting Code) is enabled.\n"
      ],
      "metadata": {
        "id": "d0bL_-Js9fA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=name,utilization.gpu,memory.total,memory.free,memory.used --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQH2yeYf49m5",
        "outputId": "02881850-7898-4a8b-e5bc-22e99ebf237a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, utilization.gpu [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]\n",
            "Tesla T4, 0 %, 15360 MiB, 15099 MiB, 3 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Commands\n",
        "\n",
        "### Setting Persistance Mode\n",
        "Keeps the GPU initalised when not being used and thus, reduces initialisation time for subsequent applications.\n",
        "\n",
        "!nvidia-smi -pm 1: Enables Persistence mode\n",
        "!nvidia-smi -pm 0: Disables Persistence mode\n",
        "\n",
        "### Setting Power Limit\n",
        "Sets power limit to optimise performance and energy consumption.\n",
        "\n",
        "!nvidia-smi -pl 100: Sets to 100W\n",
        "\n",
        "### Setting Application Clocks\n",
        "Sets Graphic (Core) and Memory clocks to the specified frequencies\n",
        "\n",
        "!nvidia-smi -ac 2505,1500\n",
        "\n",
        "### Enabling or Disabling ECC\n",
        "\n",
        "!nvidia-smi -e 1\n",
        "!nvidia-smi -e 0\n",
        "\n",
        "### Listing all GPUs\n",
        "\n",
        "!nvidia-smi --list-gpus\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkI2V_J_DwZ"
      }
    }
  ]
}